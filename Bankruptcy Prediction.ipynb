{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "   The DataSet collected over 2007-2013 describes 64 features and bankruptcy status after 1~5 year of Polish companies. Now look at the dataset first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I\n",
    "To do the necessary data analysis which includes confirming the data content, visualizing the data distribution and cleaning and preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading necessary python module\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Downloading dataset\n",
    "module_path = './DataSet/'\n",
    "filename = 'year.arff.csv'\n",
    "data = []\n",
    "for i in range(1,6):\n",
    "    data_temp = pd.read_csv(join(module_path,'{}'.format(i)+filename))\n",
    "    data.append(data_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confirming the number of companies every year\n",
    "for i in range(5):\n",
    "    print('the number of companies in {}year: {}'.format(i+1,data[i].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Glancing at the content of dataset\n",
    "data[0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the general summary of each columns\n",
    "data[1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data distribution\n",
    "pyplot.rcParams['figure.figsize'] = [24,36]\n",
    "data[0].hist(sharex=False, sharey=False, xlabelsize=12, ylabelsize=12)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another data distribution visualization\n",
    "pyplot.rcParams['figure.figsize'] = [24,100]\n",
    "data[0].plot(kind='box', subplots=True, layout=(18,4), sharex=False, sharey=False, fontsize=12)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features should be scaled using standard scaler before using them for training or testing.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaledData = []\n",
    "for i in range(5):\n",
    "    Data = data[i].as_matrix()\n",
    "    scaleddata = Data\n",
    "    scaleddata[:,:-1] = scaler.fit_transform(Data[:,:-1])\n",
    "    scaleddata = pd.DataFrame(scaleddata,columns = data[i].columns)\n",
    "    scaledData.append(scaleddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Spliting data into features and labels\n",
    "features = []\n",
    "labels = []\n",
    "for i in range(5):\n",
    "    feature, label = scaledData[i].drop('class', axis=1), scaledData[i]['class']\n",
    "    features.append(feature)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying data is highly skewed, as mentioned by the publisher of the dataset.\n",
    "for i in range(5):\n",
    "    num_of_bkr = sum(labels[i]==1)\n",
    "    num_of_hlt = sum(labels[i]==0)\n",
    "    print('the number of bankruptcy companies in {}year is {}'.format(i+1,num_of_bkr))\n",
    "    print('the number of healthy companies in {}year is {}\\n'.format(i+1,num_of_hlt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome the skew class problem i use Synthetic Minority Oversampling Technique (SMOTE) and implement this using imbalanced-learn package of scikit-learn contrib. And to avoid information leak, splitting the dataset prior to adding the generated extra data to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = [[] for i in range(4)]\n",
    "for i in range(5):\n",
    "    x_train,x_test,train,test = train_test_split(features[i],labels[i], test_size = 0.30, random_state = 0)\n",
    "    X_train.append(x_train);X_test.append(x_test);y_train.append(train);y_test.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Minority Oversampling Technique(SMOTE)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "X_resampled = []; y_resampled = []\n",
    "for i in range(5):\n",
    "    x_resampled, resampled = SMOTE().fit_sample(X_train[i], y_train[i])\n",
    "    X_resampled.append(x_resampled); y_resampled.append(resampled)\n",
    "    print(sorted(Counter(resampled).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II\n",
    "To discover bravely the intrinsical factors predicting the bankruptcy best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides the information about 64 features values and bankruptcy status after different years of different companies  so that the impact of company self-factor and economic environment factor on bankruptcy forecast should be ignored. Therefore, i just only consider the impact of original features and calculate importance of these features with different forecast durations. Doing this may be losting some information causing the prediction accuracy decreases but can let us pay more attention to the most essential things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import xgboost as xgb\n",
    "import scipy.stats as st\n",
    "#from graphviz import Digraph\n",
    "from xgboost import plot_tree\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selecting, Taking 1year data as an example. First let us confirm best parameters for XGboost model.\n",
    "xtrain = X_resampled[4];ytrain = y_resampled[4]\n",
    "xtest = X_test[4]; ytest = y_test[4]\n",
    "dtrain = xgb.DMatrix(pd.DataFrame(xtrain, columns=xtest.columns), label=ytrain, missing=np.nan)\n",
    "num_rounds = 200\n",
    "folds = StratifiedKFold(n_splits=3,random_state=1,shuffle=True) \n",
    "xgb_parameters = {\n",
    "    \"learning_rate\":[0.05,0.1,0.15],\n",
    "    \"min_child_weight\":[0.5, 1.0, 2.0, 3.0],\n",
    "    \"max_depth\":[2,4,6,8],\n",
    "    \"gamma\":st.uniform(0, 1.0),\n",
    "    \"subsample\":st.uniform(0,1.0),\n",
    "    \"colsample_bytree\":st.uniform(0.4, 0.6),\n",
    "    \"reg_lambda\": [0.1, 1.0, 5.0, 10.0]}\n",
    "clf = RandomizedSearchCV(xgb.XGBClassifier(objective='binary:logistic'),xgb_parameters,\n",
    "                         n_iter=100,cv=folds,scoring='roc_auc',n_jobs=-1)\n",
    "clf.fit(xtrain, ytrain)\n",
    "best_params = clf.best_params_\n",
    "\n",
    "classifier_1 = clf.best_estimator_\n",
    "classifier_1.fit(pd.DataFrame(xtrain, columns=xtest.columns), ytrain)\n",
    "ypred_1 = classifier_1.predict(xtest)\n",
    "\n",
    "cm_1 = confusion_matrix(ytest,ypred_1)\n",
    "accuracy_1 = (cm_1[0,0]+cm_1[1,1])/len(ytest)\n",
    "precision_1, recall_1, f_score_1, support = precision_recall_fscore_support(ytest, ypred_1, average = None)\n",
    "print(\"\\nFor Model 1 - XGBoost:\")\n",
    "print(\"Precision:\",precision_1)\n",
    "print(\"Recall:\",recall_1)\n",
    "print(\"F-Score:\",f_score_1)\n",
    "print(\"Accuracy_XGBoost:\",accuracy_1*100,'%')\n",
    "#xgb.plot_tree(classifier_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After best parameters being comfirmed, we can select top k important features\n",
    "xgb_params = best_params\n",
    "xgb_model = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "importance = xgb_model.get_fscore()\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1),reverse = True)\n",
    "Feature = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(12,8))\n",
    "xgb.plot_importance(xgb_model, height=0.8, ax=ax)\n",
    "pyplot.show()\n",
    "\n",
    "Feature = Feature[0:16]\n",
    "Features = Feature['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then We can take these features to predict the label of companies bankruptcy \n",
    "x_retrain = pd.DataFrame(xtrain, columns=xtest.columns)[Features];y_retrain = ytrain\n",
    "x_retest = xtest[Features]; y_retest = ytest\n",
    "dtrain = xgb.DMatrix(x_retrain, label=y_retrain, missing=np.nan)\n",
    "num_rounds = 200\n",
    "folds = StratifiedKFold(n_splits=3,random_state=1,shuffle=True) \n",
    "xgb_parameters = {\n",
    "    \"learning_rate\":[0.05,0.1,0.15],\n",
    "    \"min_child_weight\":[0.5, 1.0, 2.0, 3.0],\n",
    "    \"max_depth\":[2,4,6,8],\n",
    "    \"gamma\":st.uniform(0, 1.0),\n",
    "    \"subsample\":st.uniform(0,1.0),\n",
    "    \"colsample_bytree\":st.uniform(0.4, 0.6),\n",
    "    \"reg_lambda\": [0.1, 1.0, 5.0, 10.0]}\n",
    "clf = RandomizedSearchCV(xgb.XGBClassifier(objective='binary:logistic'),xgb_parameters,\n",
    "                         n_iter=100,cv=folds,scoring='f1',n_jobs=-1)\n",
    "clf.fit(x_retrain, y_retrain)\n",
    "best_params = clf.best_params_\n",
    "\n",
    "classifier_2 = clf.best_estimator_\n",
    "classifier_2.fit(x_retrain, y_retrain)\n",
    "ypred_2 = classifier_2.predict(x_retest)\n",
    "\n",
    "cm_2 = confusion_matrix(y_retest,ypred_2)\n",
    "accuracy_2 = (cm_2[0,0]+cm_2[1,1])/len(y_retest)\n",
    "precision_2, recall_2, f_score_2, support = precision_recall_fscore_support(y_retest, ypred_2, average = None)\n",
    "print(\"\\nFor Model 2 - FeaturedXGBoost:\")\n",
    "print(\"Precision:\",precision_2)\n",
    "print(\"Recall:\",recall_2)\n",
    "print(\"F-Score:\",f_score_2)\n",
    "print(\"Accuracy_FeaturedXGBoost:\",accuracy_2*100,'%')\n",
    "#xgb.plot_tree(classifier_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Better use BalancedBaggingClassifier method to predict the probability of companies bankruptcy\n",
    "x_retrain = pd.DataFrame(xtrain, columns=xtest.columns)[Features];y_retrain = ytrain\n",
    "x_retest = xtest[Features]; y_retest = ytest\n",
    "\n",
    "classifier_3 = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n",
    "                                       n_estimators = 5, bootstrap = True)\n",
    "classifier_3.fit(x_retrain,y_retrain)\n",
    "ypred_3 = classifier_3.predict(x_retest)\n",
    "\n",
    "cm_3 = confusion_matrix(y_retest,ypred_3)\n",
    "accuracy_3 = (cm_3[0,0]+cm_3[1,1])/len(y_retest)\n",
    "precision_3, recall_3, f_score_3, support = precision_recall_fscore_support(y_retest, ypred_3, average = None)\n",
    "print(\"\\nFor Model 3 - Featured-BalancedBagging:\")\n",
    "print(\"Precision:\",precision_3)\n",
    "print(\"Recall:\",recall_3)\n",
    "print(\"F-Score:\",f_score_3)\n",
    "print(\"Accuracy_Featured-BalancedBagging:\",accuracy_3*100,'%')\n",
    "#xgb.plot_tree(classifier_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Precision-Recall Curve and Giving best precision, recall and f1.\n",
    "def cvClassifier(clf, X, y, color, name, confMat = False, confMatNormalize = True):\n",
    "    sfolds = StratifiedKFold(n_splits = 5)\n",
    "    predicted_prob = np.zeros_like(y, dtype = float)\n",
    "    for train,test in sfolds.split(X, y):\n",
    "        clf.fit(X[train,:],y[train])\n",
    "        y_prob = clf.predict_proba(X[test,:])\n",
    "        predicted_prob[test] = y_prob[:,1]\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y, predicted_prob)\n",
    "    pyplot.plot(recall,precision , color=color,label = name)\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    pyplot.ylim([0.0, 1.05])\n",
    "    pyplot.xlim([0.0, 1.0])\n",
    "    pyplot.title('2-class Precision-Recall curve')\n",
    "    pyplot.legend()\n",
    "    \n",
    "    fscore = 2*(precision*recall)/(precision + recall)\n",
    "    maxFidx = np.nanargmax(fscore)\n",
    "    selP = precision[maxFidx]\n",
    "    selRecall = recall[maxFidx]\n",
    "    selThreshold = thresholds[maxFidx]\n",
    "\n",
    "    print(name + ' Best precision is : {}'.format(selP))\n",
    "    print(name +' Best recall is : {}'.format(selRecall))\n",
    "    print(name +' Best fscore is : {}\\n'.format(fscore[maxFidx]))\n",
    "\n",
    "cvClassifier(classifier_1, xtrain, ytrain, 'r','XGBoost')\n",
    "cvClassifier(classifier_2, x_retrain.as_matrix(), y_retrain, 'g','FeaturedXGBoost')\n",
    "cvClassifier(classifier_3, x_retrain.as_matrix(), y_retrain, 'b','FeaturedBalancedBagging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III\n",
    "Calculating the probability of predicting bankruptcy by weighted summation and giving an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function of calculating the probability of predicting bankruptcy\n",
    "def bankyprob(result,gamma):\n",
    "    years = len(result)\n",
    "    if years == 1:\n",
    "        prob = 1\n",
    "        label = result['label'][0]\n",
    "    else:\n",
    "        result['time'] = [pow(gamma, i+1) for i in range(years)]\n",
    "        sumweight = sum(result.apply(lambda x: x['fscore']*x['time'], axis=1))\n",
    "        bankyweight = sum(result.apply(lambda x: x['fscore']*x['time'] if x['label'] == 1 else 0, axis=1))\n",
    "        prob = bankyweight/sumweight\n",
    "        label = int(prob>=0.5)\n",
    "        if label == 0:\n",
    "            prob = 1 - prob\n",
    "    return label,prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An example: a company is given in which the first three year near the predicted bankruptcy date \n",
    "#predicting 0(Normal),1(bankruptcy) and 1(bankruptcy) respectively.\n",
    "company = []\n",
    "company.append(scaledData[0][100:101])\n",
    "company.append(scaledData[1][10000:10001])\n",
    "company.append(scaledData[2][10200:10201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting based on BalancedBagging model\n",
    "xtrain = X_resampled[0];ytrain = y_resampled[0]\n",
    "xtest = X_test[0]; ytest = y_test[0]\n",
    "Features = ['Attr21','Attr34' ,'Attr29','Attr27','Attr24' ,'Attr37' ,'Attr46',\n",
    "            'Attr5','Attr15','Attr9','Attr6','Attr1','Attr2','Attr11','Attr58','Attr3']\n",
    "x_retrain = pd.DataFrame(xtrain, columns=xtest.columns)[Features];y_retrain = ytrain\n",
    "x_retest = xtest[Features]; y_retest = ytest\n",
    "classifier = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n",
    "                                       n_estimators = 5, bootstrap = True)\n",
    "classifier.fit(x_retrain,y_retrain)\n",
    "ypred_0 = classifier.predict(company[0][Features])\n",
    "\n",
    "\n",
    "xtrain = X_resampled[1];ytrain = y_resampled[1]\n",
    "xtest = X_test[1]; ytest = y_test[1]\n",
    "Features = ['Attr27','Attr29' ,'Attr46','Attr34','Attr6' ,'Attr21' ,'Attr9',\n",
    "            'Attr24','Attr58','Attr15','Attr37','Attr5','Attr60','Attr25','Attr1','Attr61']\n",
    "x_retrain = pd.DataFrame(xtrain, columns=xtest.columns)[Features];y_retrain = ytrain\n",
    "x_retest = xtest[Features]; y_retest = ytest\n",
    "classifier = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n",
    "                                       n_estimators = 5, bootstrap = True)\n",
    "classifier.fit(x_retrain,y_retrain)\n",
    "ypred_1 = classifier.predict(company[1][Features])\n",
    "\n",
    "\n",
    "xtrain = X_resampled[2];ytrain = y_resampled[2]\n",
    "xtest = X_test[2]; ytest = y_test[2]\n",
    "Features = ['Attr5','Attr24' ,'Attr6','Attr46','Attr27' ,'Attr29' ,'Attr34',\n",
    "            'Attr9','Attr59','Attr15','Attr58','Attr21','Attr37','Attr56','Attr39','Attr41']\n",
    "x_retrain = pd.DataFrame(xtrain, columns=xtest.columns)[Features];y_retrain = ytrain\n",
    "x_retest = xtest[Features]; y_retest = ytest\n",
    "classifier = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n",
    "                                       n_estimators = 5, bootstrap = True)\n",
    "classifier.fit(x_retrain,y_retrain)\n",
    "ypred_2 = classifier.predict(company[2][Features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the probability of bankrupcty based on time factor and model \n",
    "ypred = pd.DataFrame({'fscore':[0.52,0.53,0.41],'label':list(map(int,[ypred_0,ypred_1,ypred_2]))})\n",
    "label, prob = bankyprob(ypred,0.8)\n",
    "print('label: {}\\n prob: {}'.format(label,prob))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
